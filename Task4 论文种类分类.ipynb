{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 任务说明\n",
    "\n",
    "- 学习主题：论文分类（数据建模任务），利用已有数据建模，对新论文进行类别分类；\n",
    "- 学习内容：使用论文标题完成类别分类；\n",
    "- 学习成果：学会文本分类的基本方法、`TF-IDF`等；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据处理步骤\n",
    "\n",
    "在原始arxiv论文中论文都有对应的类别，而论文类别是作者填写的。在本次任务中我们可以借助论文的标题和摘要完成：\n",
    "\n",
    "- 对论文标题和摘要进行处理；\n",
    "- 对论文类别进行处理；\n",
    "- 构建文本分类模型；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文本分类思路\n",
    "\n",
    "- 思路1：TF-IDF+机器学习分类器\n",
    "\n",
    "直接使用TF-IDF对文本提取特征，使用分类器进行分类，分类器的选择上可以使用SVM、LR、XGboost等\n",
    "\n",
    "- 思路2：FastText\n",
    "\n",
    "FastText是入门款的词向量，利用Facebook提供的FastText工具，可以快速构建分类器\n",
    "\n",
    "- 思路3：WordVec+深度学习分类器\n",
    "\n",
    "WordVec是进阶款的词向量，并通过构建深度学习分类完成分类。深度学习分类的网络结构可以选择TextCNN、TextRnn或者BiLSTM。\n",
    "\n",
    "- 思路4：Bert词向量\n",
    "\n",
    "Bert是高配款的词向量，具有强大的建模学习能力。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 具体代码实现以及讲解\n",
    "\n",
    "为了方便入门文本分类，选择思路1和思路2给大家讲解。首先完成字段读取："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-02T07:37:06.067689Z",
     "start_time": "2021-01-02T07:37:05.413594Z"
    }
   },
   "outputs": [],
   "source": [
    "# 导入所需的package\n",
    "import seaborn as sns #用于画图\n",
    "from bs4 import BeautifulSoup #用于爬取arxiv的数据\n",
    "import re #用于正则表达式，匹配字符串的模式\n",
    "import requests #用于网络连接，发送网络请求，使用域名获取对应信息\n",
    "import json #读取数据，我们的数据为json格式的\n",
    "import pandas as pd #数据处理，数据分析\n",
    "import matplotlib.pyplot as plt #画图工具"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-02T07:38:47.791291Z",
     "start_time": "2021-01-02T07:38:45.515867Z"
    }
   },
   "outputs": [],
   "source": [
    "def readArxivFile(path, columns=['id', 'submitter', 'authors', 'title', 'comments', 'journal-ref', 'doi',\n",
    "       'report-no', 'categories', 'license', 'abstract', 'versions',\n",
    "       'update_date', 'authors_parsed'], count=None):\n",
    "    '''\n",
    "    定义读取文件的函数\n",
    "        path: 文件路径\n",
    "        columns: 需要选择的列\n",
    "        count: 读取行数\n",
    "    '''\n",
    "    \n",
    "    data  = []\n",
    "    with open(path, 'r') as f: \n",
    "        for idx, line in enumerate(f): \n",
    "            if idx == count:\n",
    "                break\n",
    "                \n",
    "            d = json.loads(line)\n",
    "            d = {col : d[col] for col in columns}\n",
    "            data.append(d)\n",
    "\n",
    "    data = pd.DataFrame(data)\n",
    "    return data\n",
    "\n",
    "data = readArxivFile(r\"D:/offer/Github/DataWhale/arxiv-metadata-oai-2019.json/arxiv-metadata-oai-2019.json\", \n",
    "                     ['id', 'title', 'categories', 'abstract'],\n",
    "                    200000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了方便数据的处理，我们可以将标题和摘要拼接一起完成分类。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>categories</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0704.0297</td>\n",
       "      <td>Remnant evolution after a carbon-oxygen white ...</td>\n",
       "      <td>astro-ph</td>\n",
       "      <td>We systematically explore the evolution of t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0704.0342</td>\n",
       "      <td>Cofibrations in the Category of Frolicher Spac...</td>\n",
       "      <td>math.AT</td>\n",
       "      <td>Cofibrations are defined in the category of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0704.0360</td>\n",
       "      <td>Torsional oscillations of longitudinally inhom...</td>\n",
       "      <td>astro-ph</td>\n",
       "      <td>We explore the effect of an inhomogeneous ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0704.0525</td>\n",
       "      <td>On the Energy-Momentum Problem in Static Einst...</td>\n",
       "      <td>gr-qc</td>\n",
       "      <td>This paper has been removed by arXiv adminis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0704.0535</td>\n",
       "      <td>The Formation of Globular Cluster Systems in M...</td>\n",
       "      <td>astro-ph</td>\n",
       "      <td>The most massive elliptical galaxies show a ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                              title categories  \\\n",
       "0  0704.0297  Remnant evolution after a carbon-oxygen white ...   astro-ph   \n",
       "1  0704.0342  Cofibrations in the Category of Frolicher Spac...    math.AT   \n",
       "2  0704.0360  Torsional oscillations of longitudinally inhom...   astro-ph   \n",
       "3  0704.0525  On the Energy-Momentum Problem in Static Einst...      gr-qc   \n",
       "4  0704.0535  The Formation of Globular Cluster Systems in M...   astro-ph   \n",
       "\n",
       "                                            abstract  \n",
       "0    We systematically explore the evolution of t...  \n",
       "1    Cofibrations are defined in the category of ...  \n",
       "2    We explore the effect of an inhomogeneous ma...  \n",
       "3    This paper has been removed by arXiv adminis...  \n",
       "4    The most massive elliptical galaxies show a ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'title', 'categories', 'abstract'], dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-02T07:39:04.746931Z",
     "start_time": "2021-01-02T07:39:04.199655Z"
    }
   },
   "outputs": [],
   "source": [
    "data['text'] = data['title'] + data['abstract']\n",
    "\n",
    "data['text'] = data['text'].apply(lambda x: x.replace('\\n',' '))\n",
    "data['text'] = data['text'].apply(lambda x: x.lower())\n",
    "data = data.drop(['abstract', 'title'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于原始论文有可能有多个类别，所以也需要处理："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-02T07:39:15.639828Z",
     "start_time": "2021-01-02T07:39:15.214064Z"
    }
   },
   "outputs": [],
   "source": [
    "# 多个类别，包含子分类\n",
    "data['categories'] = data['categories'].apply(lambda x : x.split(' '))\n",
    "\n",
    "# 单个大类别，不包含子分类\n",
    "data['categories_big'] = data['categories'].apply(lambda x : [xx.split('.')[0] for xx in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [astro-ph]\n",
      "1     [math.AT]\n",
      "2    [astro-ph]\n",
      "Name: categories, dtype: object\n",
      "一共有34个大类。\n"
     ]
    }
   ],
   "source": [
    "print(data['categories'][:3])\n",
    "data['categories_big'][:3]\n",
    "# cata_big_0 = [it[0] for it in data['categories_big']]\n",
    "# cata_big_1 = [it[1] for it in data['categories_big'] if len(it)==2]\n",
    "cata_big = []\n",
    "for it in data['categories_big']:\n",
    "    cata_big.extend(it)\n",
    "#cata_big = cata_big_0 + cata_big_1\n",
    "print(f\"一共有{len(set(cata_big))}个大类。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后将类别进行编码，这里类别是多个，所以需要多编码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-02T07:39:32.136609Z",
     "start_time": "2021-01-02T07:39:31.088518Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer()\n",
    "data_label = mlb.fit_transform(data['categories_big'].iloc[:])\n",
    "#data_label是一种one-hot编码，"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "170618\n",
      "34\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(data_label.__len__())\n",
    "print(data_label[0].__len__())\n",
    "data_label[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 思路1\n",
    "\n",
    "思路1使用TFIDF提取特征，限制最多4000个单词："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-02T07:40:19.903548Z",
     "start_time": "2021-01-02T07:40:07.053896Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=4000)\n",
    "data_tfidf = vectorizer.fit_transform(data['text'].iloc[:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于这里是多标签分类，可以使用sklearn的多标签分类进行封装："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-02T07:41:42.359030Z",
     "start_time": "2021-01-02T07:41:40.804323Z"
    }
   },
   "outputs": [],
   "source": [
    "# 划分训练集和验证集\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(data_tfidf, data_label,\n",
    "                                                 test_size = 0.2,random_state = 1)\n",
    "\n",
    "# 构建多标签分类模型\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultiOutputClassifier(MultinomialNB()).fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-02T07:41:48.342696Z",
     "start_time": "2021-01-02T07:41:48.063639Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.00      0.00      0.00         1\n",
      "           2       0.00      0.00      0.00         0\n",
      "           3       0.91      0.85      0.88      3625\n",
      "           4       0.00      0.00      0.00         4\n",
      "           5       0.00      0.00      0.00         0\n",
      "           6       0.00      0.00      0.00         1\n",
      "           7       0.00      0.00      0.00         0\n",
      "           8       0.77      0.76      0.77      3801\n",
      "           9       0.84      0.89      0.86     10715\n",
      "          10       0.00      0.00      0.00         0\n",
      "          11       0.00      0.00      0.00       186\n",
      "          12       0.44      0.41      0.42      1621\n",
      "          13       0.00      0.00      0.00         1\n",
      "          14       0.75      0.59      0.66      1096\n",
      "          15       0.61      0.80      0.69      1078\n",
      "          16       0.90      0.19      0.32       242\n",
      "          17       0.53      0.67      0.59      1451\n",
      "          18       0.71      0.54      0.62      1400\n",
      "          19       0.88      0.84      0.86     10243\n",
      "          20       0.40      0.09      0.15       934\n",
      "          21       0.00      0.00      0.00         1\n",
      "          22       0.87      0.03      0.06       414\n",
      "          23       0.48      0.65      0.55       517\n",
      "          24       0.37      0.33      0.35       539\n",
      "          25       0.00      0.00      0.00         1\n",
      "          26       0.60      0.42      0.49      3891\n",
      "          27       0.00      0.00      0.00         0\n",
      "          28       0.82      0.08      0.15       676\n",
      "          29       0.86      0.12      0.21       297\n",
      "          30       0.80      0.40      0.53      1714\n",
      "          31       0.00      0.00      0.00         4\n",
      "          32       0.56      0.65      0.60      3398\n",
      "          33       0.00      0.00      0.00         0\n",
      "\n",
      "   micro avg       0.76      0.70      0.72     47851\n",
      "   macro avg       0.39      0.27      0.29     47851\n",
      "weighted avg       0.75      0.70      0.71     47851\n",
      " samples avg       0.74      0.76      0.72     47851\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\AppData\\Miniconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "D:\\AppData\\Miniconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "D:\\AppData\\Miniconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, clf.predict(x_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 思路2\n",
    "\n",
    "思路2使用深度学习模型，单词进行词嵌入然后训练。将数据集处理进行编码，并进行截断："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-02T07:57:52.147577Z",
     "start_time": "2021-01-02T07:57:52.122238Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(data['text'].iloc[:100000], \n",
    "                                                    data_label[:100000],\n",
    "                                                 test_size = 0.95,random_state = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-02T08:00:14.205263Z",
     "start_time": "2021-01-02T08:00:03.246020Z"
    }
   },
   "outputs": [],
   "source": [
    "# parameter\n",
    "max_features= 500\n",
    "max_len= 150\n",
    "embed_size=100\n",
    "batch_size = 128\n",
    "epochs = 5\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "tokens = Tokenizer(num_words = max_features)\n",
    "tokens.fit_on_texts(list(data['text'].iloc[:100000]))\n",
    "\n",
    "y_train = data_label[:100000]\n",
    "x_sub_train = tokens.texts_to_sequences(data['text'].iloc[:100000])\n",
    "x_sub_train = sequence.pad_sequences(x_sub_train, maxlen=max_len)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义模型并完成训练："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-02T08:08:55.690388Z",
     "start_time": "2021-01-02T08:00:19.943791Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "625/625 [==============================] - 1015s 2s/step - loss: 0.1023 - accuracy: 0.5250 - val_loss: 0.0709 - val_accuracy: 0.6777\n",
      "Epoch 2/5\n",
      "625/625 [==============================] - 1066s 2s/step - loss: 0.0686 - accuracy: 0.6799 - val_loss: 0.0638 - val_accuracy: 0.6999\n",
      "Epoch 3/5\n",
      "382/625 [=================>............] - ETA: 6:56 - loss: 0.0641 - accuracy: 0.6951"
     ]
    }
   ],
   "source": [
    "# LSTM model\n",
    "# Keras Layers:\n",
    "from keras.layers import Dense,Input,LSTM,Bidirectional,Activation,Conv1D,GRU\n",
    "from keras.layers import Dropout,Embedding,GlobalMaxPooling1D, MaxPooling1D, Add, Flatten\n",
    "from keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D# Keras Callback Functions:\n",
    "from keras.callbacks import Callback\n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers, callbacks\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "sequence_input = Input(shape=(max_len, ))\n",
    "x = Embedding(max_features, embed_size, trainable=True)(sequence_input)\n",
    "x = SpatialDropout1D(0.2)(x)\n",
    "x = Bidirectional(GRU(128, return_sequences=True,dropout=0.1,recurrent_dropout=0.1))(x)\n",
    "x = Conv1D(64, kernel_size = 3, padding = \"valid\", kernel_initializer = \"glorot_uniform\")(x)\n",
    "avg_pool = GlobalAveragePooling1D()(x)\n",
    "max_pool = GlobalMaxPooling1D()(x)\n",
    "x = concatenate([avg_pool, max_pool]) \n",
    "preds = Dense(34, activation=\"sigmoid\")(x)\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='binary_crossentropy',optimizer=Adam(lr=1e-3),metrics=['accuracy'])\n",
    "model.fit(x_sub_train, y_train, \n",
    "          batch_size=batch_size, \n",
    "          validation_split=0.2,\n",
    "          epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
